{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5wqKzyYpXu-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import zipfile\n",
        "import requests\n",
        "import glob as glob\n",
        "\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n",
        "from dataclasses import dataclass\n",
        "\n",
        "block_plot = False\n",
        "plt.rcParams['figure.figsize'] = (12, 9)\n",
        "SEED_VALUE = 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def system_config():\n",
        "\n",
        "    # Get list of GPUs.\n",
        "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "    print(gpu_devices)\n",
        "\n",
        "    if len(gpu_devices) > 0:\n",
        "        print('Using GPU')\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "        # If there are any gpu devices, use first gpu.\n",
        "        tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')\n",
        "\n",
        "        # Grow the memory usage as it is needed by the process.\n",
        "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "\n",
        "        # Enable using cudNN.\n",
        "        os.environ['TF_USE_CUDNN'] = \"true\"\n",
        "    else:\n",
        "        print('Using CPU')\n",
        "\n",
        "system_config()"
      ],
      "metadata": {
        "id": "hA5hZ7W5phIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, save_name):\n",
        "    url = url\n",
        "    file = requests.get(url)\n",
        "\n",
        "    open(save_name, 'wb').write(file.content)"
      ],
      "metadata": {
        "id": "dbWwDZFNpkTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(zip_file=None):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file) as z:\n",
        "            z.extractall(\"./\")\n",
        "            print(\"Extracted all\")\n",
        "    except:\n",
        "        print(\"Invalid file\")"
      ],
      "metadata": {
        "id": "TwIEz8aBpmV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_file(\n",
        "    'https://www.dropbox.com/s/7huaqeavdbz32la/dataset_ASL_150.zip?dl=1',\n",
        "    'dataset_ASL_150.zip'\n",
        ")\n",
        "\n",
        "unzip(zip_file='dataset_ASL_150.zip')"
      ],
      "metadata": {
        "id": "mO_u7czNpoBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class DatasetConfig:\n",
        "    NUM_CLASSES: int = 29\n",
        "    IMG_HEIGHT:  int = 224\n",
        "    IMG_WIDTH:   int = 224\n",
        "    CHANNELS:    int = 3\n",
        "    BATCH_SIZE:  int = 32\n",
        "    DATA_ROOT:   str = './dataset_ASL_150'\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "    BATCH_SIZE:     int   = 32\n",
        "    EPOCHS:         int   = 51\n",
        "    LEARNING_RATE:  float = 0.0001\n",
        "    CHECKPOINT_DIR: str   = './saved_models_asl.keras'"
      ],
      "metadata": {
        "id": "idzAy_-JpphT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (DatasetConfig.IMG_HEIGHT, DatasetConfig.IMG_WIDTH, DatasetConfig.CHANNELS)\n",
        "\n",
        "print('Loading model with ImageNet weights...')\n",
        "vgg16_conv_base = tf.keras.applications.vgg16.VGG16(input_shape=input_shape,\n",
        "                                                    include_top=False, # We will supply our own top.\n",
        "                                                    weights='imagenet',\n",
        "                                                   )\n",
        "# First make the convolutional base trainable.\n",
        "vgg16_conv_base.trainable = True\n",
        "print('All weights trainable, fine tuning...')"
      ],
      "metadata": {
        "id": "kaaz4NriprTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vgg16_conv_base.summary())"
      ],
      "metadata": {
        "id": "xFpJ27XLpvwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of layers to fine tune at the end of the convolutional base.\n",
        "num_layers_fine_tune = 4\n",
        "num_layers = len(vgg16_conv_base.layers)\n",
        "\n",
        "# Freeze the initial layers in the convolutional base.\n",
        "for model_layer in vgg16_conv_base.layers[:num_layers - num_layers_fine_tune]:\n",
        "    print(f\"FREEZING LAYER: {model_layer}\")\n",
        "    model_layer.trainable = False\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Configured to fine tune the last\", num_layers_fine_tune, \"convolutional layers...\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(vgg16_conv_base.summary())"
      ],
      "metadata": {
        "id": "gubyZz8Ip1Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "x = tf.keras.applications.vgg16.preprocess_input(inputs)\n",
        "\n",
        "x = vgg16_conv_base(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "# The final `Dense` layer with the number of classes.\n",
        "outputs = layers.Dense(DatasetConfig.NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# The final model.\n",
        "vgg16_model = keras.Model(inputs, outputs)\n",
        "\n",
        "print(vgg16_model.summary())"
      ],
      "metadata": {
        "id": "h-LySHHnp3B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = image_dataset_from_directory(directory=DatasetConfig.DATA_ROOT,\n",
        "                                             batch_size=TrainingConfig.BATCH_SIZE,\n",
        "                                             shuffle=True,\n",
        "                                             seed=SEED_VALUE,\n",
        "                                             label_mode='categorical',\n",
        "                                             image_size=(DatasetConfig.IMG_WIDTH, DatasetConfig.IMG_HEIGHT),\n",
        "                                             subset='training',\n",
        "                                             validation_split=0.2\n",
        "                                            )\n",
        "\n",
        "valid_dataset = image_dataset_from_directory(directory=DatasetConfig.DATA_ROOT,\n",
        "                                             batch_size=TrainingConfig.BATCH_SIZE,\n",
        "                                             shuffle=True,\n",
        "                                             seed=SEED_VALUE,\n",
        "                                             label_mode='categorical',\n",
        "                                             image_size=(DatasetConfig.IMG_WIDTH, DatasetConfig.IMG_HEIGHT),\n",
        "                                             subset='validation',\n",
        "                                             validation_split=0.2\n",
        "                                            )"
      ],
      "metadata": {
        "id": "Of6npu0Sp4sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_dataset.class_names\n",
        "\n",
        "plt.figure(figsize=(18, 10))\n",
        "\n",
        "# Assumes dataset batch_size is at least 32.\n",
        "num_rows = 4\n",
        "num_cols = 8\n",
        "\n",
        "# Here we use the take() method to retrieve just the first batch of data from the training portion of the dataset.\n",
        "for image_batch, labels_batch in train_dataset.take(1):\n",
        "\n",
        "    # For the batch of images and the associated (one-hot encoded) labels,\n",
        "    # plot each of the images in the batch and the associated ground truth labels.\n",
        "    for i in range(num_rows*num_cols):\n",
        "        ax = plt.subplot(num_rows, num_cols, i + 1)\n",
        "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
        "        truth_idx = np.nonzero(labels_batch[i].numpy())\n",
        "        plt.title(class_names[truth_idx[0][0]])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "PvvotC0ap6wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=TrainingConfig.LEARNING_RATE),\n",
        "                    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "                    metrics=['accuracy'],\n",
        "                   )"
      ],
      "metadata": {
        "id": "iqhtPOu5p7Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save best model based on highest validation_accuracy.\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=TrainingConfig.CHECKPOINT_DIR,\n",
        "                                                               save_weights_only=False,\n",
        "                                                               monitor='val_accuracy',\n",
        "                                                               mode='max',\n",
        "                                                               save_best_only=True,\n",
        "                                                              )"
      ],
      "metadata": {
        "id": "PXKmVsQWp802"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model evaluation accuracy: {vgg16_model.evaluate(valid_dataset)[1]*100.:.3f}\")"
      ],
      "metadata": {
        "id": "jOQ0Fw6Jp-Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model.\n",
        "training_results = vgg16_model.fit(train_dataset,\n",
        "                                   validation_data=valid_dataset,\n",
        "                                   epochs=TrainingConfig.EPOCHS,\n",
        "                                   callbacks=model_checkpoint_callback,\n",
        "                                  )"
      ],
      "metadata": {
        "id": "sdLLXxD7qAR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(metrics, ylabel=None, ylim=None, metric_name=None, color=None):\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 4))\n",
        "\n",
        "    if not (isinstance(metric_name, list) or isinstance(metric_name, tuple)):\n",
        "        metrics = [metrics,]\n",
        "        metric_name = [metric_name,]\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax.plot(metric, color=color[idx])\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(ylabel)\n",
        "    plt.xlim([0, TrainingConfig.EPOCHS-1])\n",
        "    plt.ylim(ylim)\n",
        "    # Tailor x-axis tick marks\n",
        "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
        "    ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
        "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
        "    plt.grid(True)\n",
        "    plt.legend(metric_name)\n",
        "    plt.show(block=block_plot)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "v5a9PGJbqD8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve training results.\n",
        "train_loss = training_results.history[\"loss\"]\n",
        "train_acc  = training_results.history[\"accuracy\"]\n",
        "valid_loss = training_results.history[\"val_loss\"]\n",
        "valid_acc  = training_results.history[\"val_accuracy\"]\n",
        "\n",
        "plot_results([ train_acc, valid_acc ],\n",
        "            ylabel=\"Accuracy\",\n",
        "            ylim = [0.5, 1.0],\n",
        "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
        "            color=[\"g\", \"b\"])\n",
        "\n",
        "max_loss = 2.0\n",
        "\n",
        "plot_results([ train_loss, valid_loss ],\n",
        "            ylabel=\"Loss\",\n",
        "            ylim = [0.0, max_loss],\n",
        "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
        "            color=[\"g\", \"b\"]);"
      ],
      "metadata": {
        "id": "iNbRTYykqFmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved model.\n",
        "# Final evaluation accuracy on the validation set.\n",
        "model = tf.keras.models.load_model(TrainingConfig.CHECKPOINT_DIR)\n",
        "print(f\"Model evaluation accuracy: {model.evaluate(valid_dataset)[1]*100.:.3f}\")"
      ],
      "metadata": {
        "id": "QyHzwGkjqGu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_wrong_predictions(dataset, checkpoint_dir=None, checkpoint_version=0):\n",
        "\n",
        "    if not checkpoint_dir:\n",
        "        checkpoint_dir = os.path.join(os.getcwd(), TrainingConfig.checkpoint_dir, f\"version_{checkpoint_version}\")\n",
        "\n",
        "    # Load saved model.\n",
        "    model = tf.keras.models.load_model(checkpoint_dir)\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    num_rows = 4\n",
        "    num_cols = 5\n",
        "    class_names = dataset.class_names\n",
        "    jdx = 0\n",
        "\n",
        "    # Evaluate all the batches.\n",
        "    for image_batch, labels_batch in dataset:\n",
        "\n",
        "        # Predictions for the current batch.\n",
        "        predictions = model.predict(image_batch)\n",
        "\n",
        "        # Loop over all the images in the current batch.\n",
        "        for idx in range(len(labels_batch)):\n",
        "\n",
        "            pred_idx = tf.argmax(predictions[idx]).numpy()\n",
        "            truth_idx = np.nonzero(labels_batch[idx].numpy())\n",
        "\n",
        "            # Plot the images with incorrect predictions\n",
        "            if pred_idx != truth_idx:\n",
        "\n",
        "                jdx += 1\n",
        "\n",
        "                if jdx > num_rows*num_cols:\n",
        "                    # Break from the loops if the maximum number of images have been plotted\n",
        "                    break\n",
        "\n",
        "                ax = plt.subplot(num_rows, num_cols, jdx)\n",
        "                title = str(class_names[truth_idx[0][0]]) + \" : \" + str(class_names[pred_idx])\n",
        "                title_obj = plt.title(title)\n",
        "                plt.setp(title_obj, color='r')\n",
        "                plt.axis(\"off\")\n",
        "                plt.imshow(image_batch[idx].numpy().astype(\"uint8\"))\n",
        "    return"
      ],
      "metadata": {
        "id": "W5f1kxbLqKVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_wrong_predictions(valid_dataset, TrainingConfig.CHECKPOINT_DIR)"
      ],
      "metadata": {
        "id": "TpvtFuM3qMFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}